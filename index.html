<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TwinVLA: Data-Efficient Bimanual Policy through Twin Single-Arm Vision-Language-Action Models">
  <meta name="keywords" content="TwinVLA, VLA, Bimanual">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    .tldr-container {
      background-color: #f5f5f5;
      border-radius: 8px;
      padding: 30px;
      margin: 40px 0;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.08);
    }
    
    .tldr-title {
      color: #333 !important;
      font-size: 2rem !important;
      font-weight: 700 !important;
      margin-bottom: 25px !important;
      display: flex;
      align-items: center;
    }
    
    .tldr-title::before {
      content: "ðŸ“Œ";
      margin-right: 12px;
      font-size: 1.8rem;
    }
    
    .tldr-video-container {
      border-radius: 6px;
      overflow: hidden;
      margin: 20px 0;
      border: 1px solid #e0e0e0;
    }
    
    .tldr-video {
      width: 100%;
      display: block;
    }
    
    .tldr-highlight {
      background-color: #e8e8e8;
      padding: 2px 6px;
      border-radius: 4px;
      font-weight: 600;
    }
    
    @media (max-width: 768px) {
      .tldr-title {
        font-size: 1.8rem !important;
      }
    }

    p {
      line-height: 1.65;
    }
  </style>
  <title>TwinVLA: Data-Efficient Bimanual Policy through Twin Single-Arm Vision-Language-Action Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jellyho.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="">
            We are working on it!
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TwinVLA:Data-Efficient Bimanual Policy through
Twin Single-Arm Vision-Language-Action Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jellyho.github.io">Hokyun Im</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jinprelude.github.io/">Euijin Jeong</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Andrey Kolobov</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Jianlong Fu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://youngwoon.github.io/">Youngwoon Lee</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University,</span>
            <span class="author-block"><sup>2</sup>Microsoft</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper video. -->
<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="tldr-container">
            <h2 class="tldr-title">TL;DR: Data-efficient, high-performance bimanual manipulation without any bimanual pre-training</h2>
            <div class="tldr-video-container">
              <video class="tldr-video" id="teaser" autoplay muted loop controls>
                <source src="./static/videos/TwinVLA_Video.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <!-- <img src="./static/images/twinvla.png" alt="Teaser"> Short video clip would be better? -->
        <div class="content has-text-justified">
          <p>
            Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual manipulation. 
            However, since most publicly available datasets focus on single-arm demonstrations, achieving strong performance on bimanual tasks typically requires collecting substantial target domain data. 
            To address this limitation, we introduce <b>TwinVLA</b>, a modular framework that fuses two pretrained single-arm VLA models into a bimanual VLA. In contrast to prior methods which naively trains train a monolithic model 
            on the mixture of single-arm and bimanual data, TwinVLA improves interpretability and data efficiency by preserving and coordinating specialized single-arm policies. 
            We evaluate TwinVLA across a diverse suite of bimanual tasks in simulation settings, and show that it outperforms prior works trained with similar data budgets, but without requiring any bimanual pretraining, thanks to its efficient design.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!--/ Paper video. -->
  </div>
</section>

<hr />

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">TwinVLA Framework</h2>
        <img src="./static/images/twinvla.png" alt="Teaser"> <!-- Short video clip would be better? -->
        <div class="content has-text-justified">
          <p>
            TwinVLA is a mixture model built from lightweight single-arm VLA (SingleVLA) experts, enabling
            bimanual manipulation without training from scratch. It explicitly coordinates both arms for efficient,
            high-performance control. TwinVLA consists of three components: (1) duplication of modules from
            SingleVLAs; (2) a Mixture-of-Experts (MoE) mechanism for shared modalities; and (3) a joint
            attention process that integrates outputs with arm-specific inputs to produce coordinated actions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- 3rd Column -->
    <div class="column">
      <div class="content">
        <h3 class="title is-4">1. Single Arm VLA</h3>
        <img src="./static/images/euijin_singlevla.png" alt="Teaser">
        <p>
          We design SingleVLA using Eagle2-1B for compact and efficient vision-language-action modeling, enabling better transfer to TwinVLA. To enhance generalization, we introduce EEF-6D control for consistent action representation and frequency matching to align control rates across datasets. These methods improve transferability to bimanual settings, and we pretrain SingleVLA on OXE data for 100K steps over 5 days.
        </p>
      </div>
    </div>
    <div class="columns is-centered">
      <!-- 2nd Column -->
      <div class="column">
        <div class="content">
          <h3 class="title is-4">2. Mixture of Experts</h3>
          <img src="./static/images/euijin_moe_and_joint.png" alt="Teaser">
          <p>
            TwinVLA shares common inputs across two backbones using a Mixture-of-Experts (MoE) to reduce redundancy. 
            It extends MoE beyond FFNs by gating shared inputs and averaging full transformer weights.
          </p>
        </div>
      </div>

      <!-- 1st Column -->
      <div class="column">
        <div class="content">
          <h3 class="title is-4">3. Joint Attention</h3>
          <img src="./static/images/euijin_attention_mask.png" alt="Teaser">
          <p></p>
          <p>
            TwinVLA shares self-attention layers across arms with a custom mask and re-weighting to enable coordination while preserving single-arm pretraining balance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<hr />

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We conduct both simulation and real-world experiments on diverse bimanual manipulation tasks
            to test the architectural advantages and training scheme of TwinVLA model.
            In both simulation and real-world robot settings,
            our method achieved comparable or even better performance than state-of-the-art bimanual policies, 
            despite using only single-arm data. Both the architecture of TwinVLA and the use of
            pretraining significantly contributed to this performance improvement.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
          <h2 class="title is-4">Tabletop Simulation</h2>
          <img src="./static/images/sim_bar.png" alt="Tabletop Simulation Results" style="max-width:100%; margin-bottom:24px;">
          <div class="carousel results-carousel">
            <div class="item item-dish">
              <video poster="" id="dish" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/dish.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-box">
              <video poster="" id="box" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/box.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-lift">
              <video poster="" id="lift" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/lift.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-shoes">
              <video poster="" id="shoes" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/shoes.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-pot">
              <video poster="" id="pot" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/pot.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
          <p>
            TwinVLA, despite being pretrained only on single-arm data, outperforms RDT-1B trained on bimanual data, demonstrating the effectiveness of single-arm prior knowledge. However, it still falls short of Ï€0, which benefits from large-scale high-quality bimanual pretraining.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-4">Real World (Anubis Robot)</h2>
        <img src="./static/images/real_bar.png" alt="Tabletop Real World Results" style="max-width:100%; margin-bottom:24px;">
        <div class="carousel results-carousel">
          <div class="item item-carrot">
            <video poster="" id="carrot" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_carrot.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-pan">
            <video poster="" id="pan" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_pan.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-kirby">
            <video poster="" id="kirby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_kirby2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-carrot">
            <video poster="" id="carrot" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_carrot2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-pan">
            <video poster="" id="pan" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_pan2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-kirby">
            <video poster="" id="kirby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/real_kirby.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            In real-world experiments, Ï€0 achieved the highest performance, with TwinVLA ranking second on average, outperforming RDT-1B and DP. TwinVLA once again shows competitive real-world performance by effectively learning bimanual tasks from single-arm data.
          </p>
        </div>
        <img src="./static/images/quali.png" alt="Tabletop Simulation Results" style="max-width:100%; margin-bottom:24px;">
        <div class="content has-text-justified">
          <p>
            <b>Carrot to Bag:</b>
            Ï€0 had the highest success rate, followed by TwinVLA. DP struggled to grasp the bag cover properly, and RDT failed mainly due to inaccurate localization of the bag opening.
          </p>
          <p>
            <b>Brush to Dustpan:</b>
            DP had trouble either grasping the brush or inserting it successfully, while RDT grasped the brush well but lacked insertion precision. TwinVLA and Ï€0 achieved the same success rate in this task.
          </p>
          <p>
            <b>Take Towel Off:</b>
            DP mostly failed to pull the doll toward the center, while the other models succeeded in doing so but varied in towel removal performance. TwinVLA struggled with removing the remaining part, whereas RDT and Ï€0 succeeded by leveraging longer action chunks to overcome multimodality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/jellyho/TwinVLA" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
            <p style="text-align: center;">
            We used template from <a href="https://nerfies.github.io/">Nerfies</a>.
            </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
